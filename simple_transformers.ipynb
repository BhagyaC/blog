{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple transformers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0d7qNxF/aEJrvzLyH+p7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhagyaC/blog/blob/main/simple_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvZa7yId-Rx4"
      },
      "source": [
        "This notebook is some simple transformer related experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qttqbTE-QdD"
      },
      "source": [
        "#installing the transofmers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq5rEQybAbfN"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlgknsD6Ad_v"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tknzr = AutoTokenizer.from_pretrained(\"distilgpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfuVW6QnA1Fm"
      },
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", output_hidden_states=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wId_q-A8BFFA"
      },
      "source": [
        "sample_text = \"I want to be a machine learning\""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcUbQsNDBNFW"
      },
      "source": [
        "#tokenise the input sententece\n",
        "input = tknzr.encode(sample_text, return_tensors=\"pt\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSmgJME6BPu4",
        "outputId": "ea735581-abf5-41d7-c291-b80fd20fc5be"
      },
      "source": [
        "input"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  40,  765,  284,  307,  257, 4572, 4673]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-CnKRRqB1LM",
        "outputId": "42609bd9-b1ba-4b2f-f8ac-3d6b8a9ab77c"
      },
      "source": [
        "output = model.generate(input, max_length=8,do_sample=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EJhAN6xCE1-",
        "outputId": "12aa5163-4f06-41c6-967b-70bf49bcfeb7"
      },
      "source": [
        "output"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  40,  765,  284,  307,  257, 4572, 4673, 4572]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VwgsOdDTCKrG",
        "outputId": "125cae65-b262-4ab1-8b14-532c60ce51a5"
      },
      "source": [
        "tknzr.decode(output[0])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I want to be a machine learning machine'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPJH2XHWCt0N"
      },
      "source": [
        "Here the predicted word after machine learning is again machine which is not so good. Let us see how the word to vectors works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MIDqR3UCYxC",
        "outputId": "bd507484-caca-49ca-fbec-65d21391fb27"
      },
      "source": [
        "input"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  40,  765,  284,  307,  257, 4572, 4673]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkQ2qqlIDAWI"
      },
      "source": [
        "#this is the vectorised input now let us see how they look them converted into tokends"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXUek9wyDISE",
        "outputId": "6d08f004-9365-4b20-b1f3-29108e119074"
      },
      "source": [
        "tknzr.convert_ids_to_tokens(input[0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'Ġwant', 'Ġto', 'Ġbe', 'Ġa', 'Ġmachine', 'Ġlearning']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxCvtg-9DVnX"
      },
      "source": [
        "Earlier in our blog we learned that the embedding vector size will be different for differntr models. to know what all are the dimention we can use \"wte\" function which will return the number of tokens in the vocabulary and the dimenstion of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4IT6W71DMmQ",
        "outputId": "fba2bbb7-b8ad-4f83-fbd6-471f47960b76"
      },
      "source": [
        "model.transformer.wte"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFQxJu8TDwhh"
      },
      "source": [
        "Previously when we converted the input to the tokens we got some values correspoding to each work right? Let us look into the embedding for those tokens, for example 4572 is for machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPGCTS8vEf9R"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DloQLdZDpsi",
        "outputId": "bb7996d2-0128-437a-f5f5-696e12959554"
      },
      "source": [
        "model.transformer.wte(torch.tensor(4572))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 5.0140e-02, -3.9427e-02,  5.8163e-02,  8.9592e-03, -4.7968e-02,\n",
              "        -7.3724e-02, -3.5826e-01,  2.5049e-02, -7.0663e-02,  1.2497e-02,\n",
              "         2.0787e-01, -1.0791e-01,  4.5138e-03,  1.1096e-01,  8.3473e-02,\n",
              "         5.2399e-02,  4.6622e-02,  4.5274e-02,  1.0219e-01,  8.2677e-02,\n",
              "        -3.0611e-03, -1.3619e-01,  2.9104e-02,  4.0383e-02, -1.0709e-01,\n",
              "         9.5735e-02,  2.4363e-01,  1.6260e-01, -2.8777e-02,  1.5653e-01,\n",
              "         7.6510e-02,  7.5093e-02,  3.0166e-01,  4.8214e-03, -5.8404e-02,\n",
              "         4.7402e-02, -3.1573e-01,  1.2504e-01,  6.4323e-02, -1.9254e-01,\n",
              "         6.3028e-02,  1.7854e-02, -1.1224e-01,  3.2452e-04,  1.0550e-01,\n",
              "         7.2170e-02,  1.8547e-01, -1.5653e-02, -1.7186e-01, -9.1829e-02,\n",
              "        -3.1259e-02,  2.7338e-02,  8.9431e-02, -1.9080e-02, -9.0623e-02,\n",
              "        -4.1314e-01,  2.2463e-01,  3.5204e-02, -4.4846e-02,  4.3254e-02,\n",
              "         9.0543e-02,  1.7040e-01,  9.8413e-02,  5.0725e-02,  3.3914e-02,\n",
              "         2.6470e-01, -2.3209e-01, -1.3297e-01,  1.4601e-01, -7.5699e-03,\n",
              "         3.5272e-02, -4.8243e-02, -1.5633e-02, -2.3876e-01, -2.3557e-01,\n",
              "         2.8183e-01, -3.3851e-02,  3.7344e-01,  1.1043e-01, -1.0521e-01,\n",
              "        -2.0129e-01, -3.6493e-02, -1.9439e-01,  6.6918e-02, -5.5027e-02,\n",
              "         1.2529e-01, -2.1519e-01,  1.5255e-02,  1.3522e-01, -1.1051e-01,\n",
              "        -1.2566e-01,  3.9981e-02,  2.7478e-01, -1.0311e-02, -1.0597e-01,\n",
              "         9.3483e-02,  1.2595e-02, -3.4491e-02, -3.5718e-02, -9.5109e-02,\n",
              "         1.0486e-01, -2.0356e-01, -2.3045e-01, -2.3431e-01,  1.1965e-01,\n",
              "        -2.4219e-02, -1.4874e-01, -2.1720e-01,  1.5541e-02, -4.2147e-03,\n",
              "        -3.1770e-02, -8.5066e-03, -2.5028e-02,  2.1237e-01, -5.9571e-02,\n",
              "         1.6487e-02,  1.5574e-01,  1.9513e-02,  1.9200e-01, -7.1404e-02,\n",
              "         1.4369e-03, -1.2266e-01, -5.0194e-03, -1.0453e-01,  7.5083e-02,\n",
              "        -2.8851e-01, -1.0429e-01, -4.4701e-02,  9.0168e-03,  3.0020e-02,\n",
              "         4.3942e-02, -1.0655e-01,  8.0772e-02, -4.7924e-02, -1.1876e-01,\n",
              "        -5.9323e-02, -2.5663e-01,  1.3395e-01,  3.3539e-02, -1.1156e-01,\n",
              "         1.5286e-01,  1.2234e-01,  2.4647e-01,  1.7079e-01,  1.2994e-02,\n",
              "        -2.9014e-01,  1.9484e-01, -1.5184e-01, -1.0529e-01,  5.8786e-02,\n",
              "         5.8731e-02, -7.3025e-02, -1.4024e-01, -2.7192e-02, -3.9684e-02,\n",
              "        -1.2915e-01, -1.6562e-01,  6.6375e-02, -2.3726e-02,  2.4541e-01,\n",
              "        -1.1137e-01, -9.9806e-02,  2.3258e-01, -1.3808e-01, -1.8186e-01,\n",
              "        -6.7951e-02,  5.2775e-02,  1.4476e-01,  7.2690e-02,  5.4045e-02,\n",
              "        -2.0934e-03, -1.0101e-01, -1.0371e-01,  8.4654e-02,  1.4712e-01,\n",
              "         7.3197e-02,  8.4068e-02, -1.5852e-02, -1.9129e-02, -7.4579e-02,\n",
              "         9.6624e-02, -1.0427e-02, -1.2964e-01, -1.6249e-01,  1.9522e-02,\n",
              "         4.7124e-02,  2.2117e-04,  1.5141e-01,  1.5945e-02, -4.7801e-02,\n",
              "         7.4897e-02,  8.6098e-02,  1.4936e-01,  8.1812e-02, -1.6102e-01,\n",
              "         2.5884e-01, -1.2389e-01,  1.4379e-02, -7.4752e-02,  4.1162e-02,\n",
              "         2.2252e-02,  4.7284e-02, -1.2864e-01,  9.3074e-02, -1.1271e-01,\n",
              "        -1.2183e-01,  6.1962e-02,  6.6869e-02, -6.9142e-02, -2.0971e-01,\n",
              "         2.0100e-01,  2.0513e-01,  2.7815e-01, -1.2803e-02,  1.0912e-01,\n",
              "        -4.4154e-02,  1.9108e-01, -2.2170e-01, -7.8312e-02, -2.1528e-02,\n",
              "        -1.0488e-01, -9.1944e-03, -1.0100e-01, -4.8800e-02, -2.1150e-01,\n",
              "        -2.4014e-01, -2.5546e-02, -1.0559e-01,  2.0696e-01, -9.1201e-02,\n",
              "         2.7537e-01,  2.9109e-02, -4.0264e-01, -1.1250e-01, -7.8612e-02,\n",
              "        -1.6957e-01, -3.0379e-02, -2.2815e-02,  7.1153e-02,  3.8556e-03,\n",
              "         4.9816e-02, -2.4837e-01, -3.4642e-01,  8.6411e-02,  4.3973e-02,\n",
              "        -1.7880e-01, -1.4976e-01,  5.2356e-02,  1.8423e-02,  9.0606e-02,\n",
              "         1.9730e-01,  1.8755e-01,  1.5079e-01, -1.4428e-02, -3.8378e-02,\n",
              "        -8.7298e-02,  8.4070e-02,  1.1030e-01, -1.2799e-01, -1.2366e-02,\n",
              "         1.5648e-01, -1.1722e-01,  7.1960e-02,  4.2461e-03,  7.5277e-02,\n",
              "        -7.3443e-02,  1.6866e-02, -1.4658e-01, -1.0421e-02, -1.6374e-02,\n",
              "         3.6947e-01, -1.5348e-01,  4.9457e-03, -4.6464e-02,  7.1711e-02,\n",
              "        -1.0927e-01,  6.7986e-03, -1.3500e-01,  3.1342e-02, -2.0030e-01,\n",
              "        -4.4837e-02, -7.9078e-02, -1.9920e-02, -1.3275e-01, -6.7210e-02,\n",
              "        -4.7157e-02, -1.2748e-01, -1.0453e-01,  1.3866e-03,  1.4295e-01,\n",
              "         1.2191e-01,  1.4077e-01, -1.3271e-01, -1.7276e-03,  2.3407e-03,\n",
              "         3.6917e-02,  2.0248e-03,  1.6353e-02, -2.5169e-02,  4.9885e-02,\n",
              "         2.3833e-02,  2.2956e-01,  5.2411e-02,  1.0468e-01, -1.2400e-01,\n",
              "        -3.1972e-02, -5.5389e-02,  1.9534e-01,  7.2802e-02,  1.5766e-01,\n",
              "         1.2135e-01,  3.5952e-02,  8.6297e-02,  5.4937e-02,  1.7755e-01,\n",
              "        -1.5473e-01, -1.0071e-01, -7.3080e-02, -1.3720e-01,  1.5961e-01,\n",
              "         1.5116e-01, -2.1017e-02,  2.8719e-02,  4.3232e-02, -8.3117e-02,\n",
              "         9.4705e-02, -2.2096e-01, -9.4323e-02,  5.5030e-02, -5.2288e-02,\n",
              "        -1.1423e-01,  6.5028e-02, -7.1840e-02, -5.4806e-02, -3.4362e-02,\n",
              "         2.1615e-01,  6.0188e-02, -2.1410e-01,  1.1757e-01, -8.9545e-02,\n",
              "         8.0084e-02,  1.4645e-01,  2.5983e-01,  2.9348e-02,  2.7273e-02,\n",
              "         8.6798e-02, -4.2913e-02,  2.6657e-01, -1.4079e-02, -9.8701e-02,\n",
              "        -2.0353e-02, -1.1603e-01,  2.6421e-01, -1.1612e-01, -1.4886e-01,\n",
              "        -2.7139e-02,  4.7687e-02, -1.4410e-01, -7.4584e-02, -3.0585e-01,\n",
              "        -5.7407e-02, -1.3809e-02, -1.9601e-01, -2.9958e-01,  1.7200e-01,\n",
              "         1.6060e-01, -9.0182e-02,  7.9834e-02, -1.2145e-01,  4.4883e-02,\n",
              "         1.4817e-01,  1.6540e-02, -2.4832e-01,  3.2109e-02,  1.7049e-01,\n",
              "        -1.1740e-02,  4.9906e-02, -1.8487e-01,  8.5412e-02,  1.0954e-01,\n",
              "         4.3813e-02, -1.1268e-01,  1.8081e-02,  8.5904e-02,  8.4079e-02,\n",
              "        -1.5614e-01,  8.9402e-02,  9.1586e-02, -1.2508e-01,  1.0410e-01,\n",
              "        -6.9934e-02, -1.5142e-02, -2.9747e-01,  1.1083e-01,  1.4036e-02,\n",
              "        -1.4199e-01, -5.0821e-02, -6.0781e-02,  1.0844e-01,  3.3768e-02,\n",
              "         2.1900e-03,  7.1317e-02, -1.1520e-01, -7.5656e-02,  4.2575e-02,\n",
              "        -2.2277e-01, -4.5165e-02, -1.2507e-01, -1.6603e-01, -2.2229e-03,\n",
              "        -1.4674e-01, -1.9644e-01, -9.6954e-02,  5.1295e-02, -9.6254e-02,\n",
              "        -4.0638e-02, -2.0122e-01, -4.3356e-02,  7.2855e-02, -2.8138e-02,\n",
              "         4.5798e-02,  6.4754e-02, -4.8631e-02, -1.2788e-01,  4.9696e-02,\n",
              "         1.2367e-01, -7.9502e-02, -7.4811e-02, -1.7660e-01, -4.4871e-02,\n",
              "        -2.5739e-01, -3.9474e-02, -1.3873e-01, -2.2833e-01,  1.0875e-01,\n",
              "        -2.4634e-01, -1.4361e-01, -3.2089e-02,  1.0491e-01, -1.5904e-01,\n",
              "         3.2913e-02, -1.3722e-01, -2.2598e-01,  5.7802e-02,  6.7065e-02,\n",
              "        -9.5672e-02,  2.2747e-04, -4.3351e-02,  2.9259e-01, -5.2534e-02,\n",
              "        -3.6587e-02,  1.1987e-01, -2.1934e-02,  1.4079e-01,  8.6515e-02,\n",
              "        -1.3464e-02,  6.2535e-02,  3.7499e-02,  3.4566e-02, -4.4873e-02,\n",
              "        -3.7243e-02, -2.9039e-01,  9.7910e-02,  1.0186e-01, -1.0932e-01,\n",
              "         3.4043e-02, -4.1414e-02,  9.1043e-02, -1.8476e-01, -1.7685e-01,\n",
              "        -2.5722e-02,  7.7342e-02, -7.8762e-02, -5.5250e-02, -3.7228e-01,\n",
              "         2.0806e-01,  1.7354e-01, -1.8065e-01,  2.4998e-02,  1.7020e-01,\n",
              "        -4.9289e-03, -1.9452e-02,  2.3539e-01, -6.2066e-02,  1.1508e-01,\n",
              "        -1.4285e-01,  2.5450e-02,  1.4421e-01, -2.7648e-01, -6.0216e-02,\n",
              "         2.0097e-01,  4.1927e-02,  1.3088e-01,  4.5790e-02, -8.1820e-02,\n",
              "         3.6743e-02, -2.0450e-01,  1.4571e-01, -2.3658e-01,  7.4297e-02,\n",
              "        -3.5443e-02, -1.3740e-01,  2.3482e-01,  1.6683e-01, -5.5435e-02,\n",
              "         5.9399e-02,  1.5336e-01, -6.0425e-02,  7.3047e-02,  8.4337e-02,\n",
              "        -1.7508e-01,  1.4493e-01, -1.3551e-01,  1.3502e-01, -1.1291e-01,\n",
              "         1.5581e-02, -1.7032e-01,  1.7371e-01, -2.8825e-01,  1.3004e-01,\n",
              "         1.7212e-01,  2.3729e-02, -7.2716e-02, -2.5495e-03,  4.6871e-02,\n",
              "         2.1096e-02,  3.7553e-01,  1.6076e-01,  7.4228e-02, -5.0138e-02,\n",
              "         2.0448e-01, -7.2152e-02,  2.2112e-01, -4.0357e-02,  6.7850e-02,\n",
              "         1.0881e-02,  3.1807e-02, -7.9060e-03,  1.1354e-01, -3.1294e-02,\n",
              "        -4.7659e-02,  3.1361e-02,  1.6371e-01,  1.2499e-01,  1.3341e-01,\n",
              "        -2.7469e-01,  7.6896e-02, -2.2204e-01, -1.4557e-01,  2.0573e-02,\n",
              "        -1.9577e-02, -1.6044e-01,  2.4923e-02,  8.2676e-02,  1.8388e-01,\n",
              "         1.6994e-01, -4.4159e-02,  1.3242e-01, -2.6623e-01, -6.1493e-02,\n",
              "        -1.2592e-02, -1.7356e-01, -5.3037e-02,  4.1239e-03,  1.3963e-01,\n",
              "        -1.6570e-01, -9.7874e-02, -1.3495e-01,  8.2218e-03,  3.1223e-02,\n",
              "        -2.0397e-02,  1.4531e-01,  1.2312e-01, -5.7427e-02,  5.1181e-02,\n",
              "         1.4933e-02,  1.3333e-01,  2.4455e-02, -2.6824e-02,  1.6786e-01,\n",
              "         1.1758e-01, -3.5184e-02,  2.8479e-03,  1.8418e-01,  3.2100e-02,\n",
              "        -5.5668e-03, -9.9319e-02,  1.0381e-01, -1.0203e-01,  2.5889e-01,\n",
              "        -1.5313e-01,  3.0769e-01,  1.2032e-01,  6.3787e-02,  4.2627e-02,\n",
              "        -1.1655e-03,  2.9102e-01, -7.8425e-02, -1.0005e-01, -1.6831e-01,\n",
              "         5.0210e-02,  7.7073e-02,  4.2901e-02, -1.1276e-01,  1.5057e-01,\n",
              "        -3.6314e-01,  1.2779e-01, -1.3652e-01, -1.7917e-01,  8.2407e-02,\n",
              "        -4.8383e-02, -8.8195e-02,  2.7460e-01,  8.3994e-02,  1.3330e-01,\n",
              "        -2.2430e-02, -3.1673e-01, -8.1000e-02, -1.0735e-01, -7.3491e-02,\n",
              "         2.2987e-01,  4.8390e-02, -1.9111e-01, -3.2664e-02, -3.5958e-02,\n",
              "         2.4942e-01,  1.7652e-01,  1.4336e-01,  4.2277e-02,  1.1621e-01,\n",
              "        -9.9916e-02, -2.0572e-01,  7.5046e-02,  6.2632e-02,  1.0815e-02,\n",
              "         2.6771e-01,  2.7070e-02,  1.9105e-01,  1.2086e-01,  1.3427e-01,\n",
              "        -2.0845e-02,  4.3747e-02,  9.8220e-02,  6.9798e-02,  1.8462e-01,\n",
              "        -3.4557e-02, -2.0368e-01,  2.0792e-01, -8.3972e-02,  2.0086e-01,\n",
              "         7.7093e-02,  9.6442e-02,  2.0751e-01,  5.3284e-02, -8.2795e-02,\n",
              "        -1.0113e-01,  1.6001e-01,  4.8776e-02,  1.5578e-02, -1.6249e-01,\n",
              "        -9.8619e-02,  5.5699e-03,  2.5013e-02, -2.1238e-01,  1.9120e-01,\n",
              "        -9.7435e-03,  1.6736e-01,  2.0249e-01,  1.4962e-03, -2.8060e-02,\n",
              "        -6.2270e-03, -2.7877e-02, -6.9831e-02,  9.6172e-02, -8.6480e-03,\n",
              "        -1.6129e-02,  3.4559e-02, -1.6729e-01,  4.1808e-02, -1.6205e-01,\n",
              "         1.6736e-01, -1.4571e-01, -5.8885e-02, -1.4892e-01,  5.4931e-02,\n",
              "        -4.9445e-02,  3.3737e-02, -2.3282e-02, -3.2209e-01, -8.9939e-02,\n",
              "        -2.4038e-01,  5.0583e-03, -1.6782e-01,  2.1745e-01,  2.6650e-02,\n",
              "        -7.8930e-02, -1.6280e-01, -4.5376e-02,  4.0306e-02,  3.6274e-02,\n",
              "        -2.4158e-01, -3.1307e-02,  1.1275e-01,  2.7730e-01,  2.3610e-01,\n",
              "         2.3782e-01,  1.2252e-01, -7.7796e-02,  5.4877e-02,  1.5856e-01,\n",
              "         1.4235e-01,  1.2893e-01,  1.6088e-01,  8.7830e-02, -1.2132e-01,\n",
              "         1.7731e-01, -1.7211e-02,  1.2872e-01,  4.7023e-02, -2.9804e-01,\n",
              "        -1.7127e-02,  2.2972e-01,  1.7835e-01, -2.1767e-01, -1.7241e-02,\n",
              "         2.0583e-01,  2.9882e-02,  2.5217e-03,  6.4826e-02,  3.8207e-02,\n",
              "         9.9915e-02,  1.2889e-01,  1.2190e-02, -4.5102e-03, -6.7744e-02,\n",
              "        -3.2380e-04,  1.7588e-01, -1.0051e-01,  4.5199e-02,  1.1047e-01,\n",
              "         8.1091e-02,  1.5582e-01,  6.3283e-03,  7.4950e-02,  3.2744e-02,\n",
              "         1.2221e-01, -1.2792e-02, -8.5657e-02, -3.4834e-02, -7.9351e-02,\n",
              "        -5.2456e-02,  4.4030e-02, -1.0572e-01, -3.4450e-02,  9.8226e-02,\n",
              "        -1.9743e-02, -2.3486e-01, -3.0554e-03,  1.4334e-01, -1.5663e-01,\n",
              "        -1.7039e-01, -1.3250e-01, -1.4928e-01, -2.3510e-01, -2.7286e-01,\n",
              "         1.5002e-01, -2.0706e-01,  2.1525e-03], grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a54gx88EtyY"
      },
      "source": [
        "Now this is the vector representation for word machine using transformer algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibOebr_HEW8I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}